"""RAG Chain for response generation with source attribution.

Synthesizes responses from:
1. Retrieved context documents
2. Tool call results
3. Conversation history
4. Domain-specific system prompts
"""

import logging
import os
from dataclasses import dataclass, field
from typing import Optional

logger = logging.getLogger(__name__)


@dataclass
class SourceAttribution:
    """Attribution to a source document."""

    document: str  # Document name/ID
    section: Optional[str] = None
    relevance_score: float = 0.0
    excerpt: str = ""


@dataclass
class GeneratedResponse:
    """Response generated by the RAG chain."""

    text: str
    sources: list[SourceAttribution]
    tokens_used: int
    model: str
    finish_reason: str = "stop"


class RAGChain:
    """RAG Chain for generating grounded responses.

    Pipeline:
    1. Format context from retrieved documents
    2. Format tool results as structured data
    3. Build conversation context from history
    4. Construct prompt with system context
    5. Generate response via LLM
    6. Extract and format source attributions

    Features:
    - Source attribution in response
    - Token counting for usage tracking
    - Confidence scoring based on grounding
    - Support for streaming responses
    """

    def __init__(
        self,
        model: str = "gpt-4o",
        temperature: float = 0.3,
        max_tokens: int = 1024,
    ):
        self.model = os.getenv("OPENAI_MODEL", model)
        self.temperature = temperature
        self.max_tokens = max_tokens
        self._client = None

    async def _get_client(self):
        """Get or create OpenAI client."""
        if self._client is None:
            try:
                from openai import AsyncOpenAI
                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    raise ValueError("OPENAI_API_KEY environment variable required")
                self._client = AsyncOpenAI(api_key=api_key)
            except ImportError:
                raise ImportError("openai package required: pip install openai")
        return self._client

    async def generate(
        self,
        query: str,
        context_docs: list,
        tool_results: Optional[list[dict]] = None,
        conversation_history: Optional[object] = None,
        system_context: Optional[str] = None,
    ) -> GeneratedResponse:
        """Generate a response using RAG pipeline.

        Args:
            query: User's query
            context_docs: Retrieved documents from knowledge base
            tool_results: Results from tool calls
            conversation_history: Previous conversation turns
            system_context: Domain-specific system prompt

        Returns:
            GeneratedResponse with text, sources, and metadata
        """
        # Build prompt components
        formatted_context = self._format_context(context_docs)
        formatted_tools = self._format_tool_results(tool_results or [])
        messages = self._build_messages(
            query=query,
            context=formatted_context,
            tool_data=formatted_tools,
            history=conversation_history,
            system_context=system_context,
        )

        # Generate response
        try:
            client = await self._get_client()
            response = await client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )

            # Extract response text
            text = response.choices[0].message.content or ""
            tokens_used = response.usage.total_tokens if response.usage else 0
            finish_reason = response.choices[0].finish_reason or "stop"

            # Extract source attributions
            sources = self._extract_attributions(text, context_docs)

            logger.info(
                "Response generated",
                extra={
                    "tokens_used": tokens_used,
                    "sources_count": len(sources),
                    "model": self.model,
                },
            )

            return GeneratedResponse(
                text=text,
                sources=sources,
                tokens_used=tokens_used,
                model=self.model,
                finish_reason=finish_reason,
            )

        except Exception as e:
            logger.error(f"Response generation failed: {e}")
            # Return fallback response
            return GeneratedResponse(
                text=self._generate_fallback_response(query, context_docs),
                sources=[],
                tokens_used=0,
                model=self.model,
                finish_reason="error",
            )

    def _format_context(self, docs: list) -> str:
        """Format retrieved documents as context string."""
        if not docs:
            return "No relevant documents found in knowledge base."

        context_parts = []
        for i, doc in enumerate(docs, 1):
            # Handle both dict and object documents
            if hasattr(doc, 'content'):
                content = doc.content
                source = getattr(doc, 'source_path', f'Document {i}')
                score = getattr(doc, 'relevance_score', 0)
            else:
                content = doc.get('content', doc.get('text', str(doc)))
                source = doc.get('source_path', doc.get('source', f'Document {i}'))
                score = doc.get('relevance_score', 0)

            # Truncate long documents
            if len(content) > 1500:
                content = content[:1500] + "..."

            context_parts.append(
                f"[Source {i}: {source} (relevance: {score:.2f})]\n{content}"
            )

        return "\n\n---\n\n".join(context_parts)

    def _format_tool_results(self, tool_results: list[dict]) -> str:
        """Format tool results as structured context."""
        if not tool_results:
            return ""

        parts = ["### Live Data from Tools ###"]

        for i, result in enumerate(tool_results, 1):
            if isinstance(result, dict):
                # Format key-value pairs
                formatted = "\n".join(
                    f"  {k}: {v}" for k, v in result.items()
                    if not k.startswith("_")
                )
                parts.append(f"Tool Result {i}:\n{formatted}")
            else:
                parts.append(f"Tool Result {i}: {result}")

        return "\n\n".join(parts)

    def _build_messages(
        self,
        query: str,
        context: str,
        tool_data: str,
        history: Optional[object],
        system_context: Optional[str],
    ) -> list[dict]:
        """Build message list for LLM call."""
        messages = []

        # System message
        system_prompt = self._build_system_prompt(system_context)
        messages.append({"role": "system", "content": system_prompt})

        # Add conversation history
        if history and hasattr(history, 'messages'):
            for msg in history.messages[-6:]:  # Last 3 turns (6 messages)
                role = msg.role if hasattr(msg, 'role') else msg.get('role', 'user')
                content = msg.content if hasattr(msg, 'content') else msg.get('content', '')
                messages.append({"role": role, "content": content})

        # Build user message with context
        user_content = self._build_user_prompt(query, context, tool_data)
        messages.append({"role": "user", "content": user_content})

        return messages

    def _build_system_prompt(self, custom_context: Optional[str]) -> str:
        """Build system prompt for telecom domain."""
        base_prompt = """You are an expert telecom operations AI assistant. Your role is to help network engineers with:
- Diagnosing network issues using live data and documentation
- Explaining procedures and best practices
- Interpreting metrics and SLA compliance
- Recommending actions based on evidence

Guidelines:
1. ALWAYS ground your responses in the provided context documents and tool data
2. Cite sources when making factual claims: [Source N]
3. If information is not in the provided context, say so clearly
4. For numerical data (latency, throughput, etc.), use exact values from sources
5. Never guess about network configurations - recommend checking if unsure
6. Flag potential issues that require human attention

Response format:
- Be concise and technical
- Use bullet points for lists
- Include specific metrics when available
- End with actionable recommendations when appropriate"""

        if custom_context:
            return f"{base_prompt}\n\n{custom_context}"
        return base_prompt

    def _build_user_prompt(
        self,
        query: str,
        context: str,
        tool_data: str,
    ) -> str:
        """Build user prompt with context and query."""
        parts = []

        if context and context != "No relevant documents found in knowledge base.":
            parts.append(f"### Retrieved Documentation ###\n{context}")

        if tool_data:
            parts.append(tool_data)

        parts.append(f"### User Query ###\n{query}")

        return "\n\n".join(parts)

    def _extract_attributions(
        self,
        response_text: str,
        docs: list,
    ) -> list[SourceAttribution]:
        """Extract source attributions from response and documents."""
        attributions = []
        seen_sources = set()

        # Look for explicit source citations [Source N] or [N]
        import re
        citations = re.findall(r'\[(?:Source\s*)?(\d+)\]', response_text)

        for citation in citations:
            try:
                idx = int(citation) - 1  # Convert to 0-indexed
                if 0 <= idx < len(docs):
                    doc = docs[idx]

                    # Get source info
                    if hasattr(doc, 'source_path'):
                        source = doc.source_path
                        score = getattr(doc, 'relevance_score', 0.5)
                        content = getattr(doc, 'content', '')[:200]
                    else:
                        source = doc.get('source_path', doc.get('source', f'Document {idx+1}'))
                        score = doc.get('relevance_score', 0.5)
                        content = doc.get('content', doc.get('text', ''))[:200]

                    if source not in seen_sources:
                        seen_sources.add(source)
                        attributions.append(SourceAttribution(
                            document=source,
                            section=None,
                            relevance_score=score,
                            excerpt=content,
                        ))
            except (ValueError, IndexError):
                continue

        # If no explicit citations, attribute to top docs used
        if not attributions and docs:
            for doc in docs[:3]:  # Top 3 docs
                if hasattr(doc, 'source_path'):
                    source = doc.source_path
                    score = getattr(doc, 'relevance_score', 0.5)
                else:
                    source = doc.get('source_path', doc.get('source', 'Unknown'))
                    score = doc.get('relevance_score', 0.5)

                if source not in seen_sources:
                    seen_sources.add(source)
                    attributions.append(SourceAttribution(
                        document=source,
                        relevance_score=score,
                    ))

        return attributions

    def _generate_fallback_response(
        self,
        query: str,
        docs: list,
    ) -> str:
        """Generate a fallback response when LLM fails."""
        if docs:
            doc_list = ", ".join(
                getattr(d, 'source_path', d.get('source', 'document'))[:50]
                for d in docs[:3]
            )
            return (
                f"I found relevant information in: {doc_list}. "
                "However, I encountered an issue generating a detailed response. "
                "Please try again or consult the documents directly."
            )
        return (
            "I couldn't find relevant information in the knowledge base for your query. "
            "Please rephrase or provide more details, or consider escalating to "
            "a human operator for assistance."
        )

    async def close(self):
        """Clean up resources."""
        if self._client:
            await self._client.close()
            self._client = None
